{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.18.5\n",
      "  Using cached numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.2\n",
      "    Uninstalling numpy-1.20.2:\n",
      "      Successfully uninstalled numpy-1.20.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.4.1 requires gast==0.3.3, but you have gast 0.4.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires grpcio~=1.32.0, but you have grpcio 1.37.1 which is incompatible.\n",
      "tensorflow 2.4.1 requires numpy~=1.19.2, but you have numpy 1.18.5 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.18.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U numpy==1.18.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from PIL import Image\n",
    "def imread(path):\n",
    "    from PIL import Image\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "def imresize(img, size):\n",
    "    from PIL import Image\n",
    "    return np.array(Image.fromarray(img).resize(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 18 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx =[0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches =len(t)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=np.array(Image.fromarray((image * 255).astype(np.uint8)).resize((120, 120)).convert('RGB'))                    \n",
    "                    batch_data[folder,idx,:,:,0] =  (image[:,:,0])/255#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] =  (image[:,:,1])/255#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] =  (image[:,:,2])/255#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if(len(t)%batch_size)!=0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(len(t)%batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=np.array(Image.fromarray((image * 255).astype(np.uint8)).resize((120, 120)).convert('RGB'))\n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255#normalise and feed in the image # divide by 255.0\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1 # OHE\n",
    "            yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30  # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout,GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "#write your model here\n",
    "model = Sequential()\n",
    "model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(18,120,120,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same', input_shape=(18,120,120,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same', input_shape=(18,120,120,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 18, 120, 120, 64)  5248      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 18, 120, 120, 64)  256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 18, 120, 120, 64)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 9, 60, 120, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 9, 60, 120, 128)   221312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 9, 60, 120, 128)   512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9, 60, 120, 128)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 4, 30, 60, 128)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 4, 30, 60, 256)    884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 30, 60, 256)    1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 30, 60, 256)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 2, 15, 60, 256)    0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 460800)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 460800)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               235930112 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 237,046,021\n",
      "Trainable params: 237,045,125\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser =  optimizers.Adam(lr=0.001) # write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 20\n",
      "Epoch 1/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 309.6677 - categorical_accuracy: 0.2715Source path =  Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 136s 4s/step - loss: 306.5118 - categorical_accuracy: 0.2733 - val_loss: 82.3638 - val_categorical_accuracy: 0.1600\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 51s 2s/step - loss: 51.1940 - categorical_accuracy: 0.4178 - val_loss: 6.1388 - val_categorical_accuracy: 0.2000\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 7.1393 - categorical_accuracy: 0.4213 - val_loss: 2.9288 - val_categorical_accuracy: 0.2500\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 3.1149 - categorical_accuracy: 0.3855 - val_loss: 2.0094 - val_categorical_accuracy: 0.2100\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.4825 - categorical_accuracy: 0.3728 - val_loss: 3.4218 - val_categorical_accuracy: 0.1900\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 51s 1s/step - loss: 1.4715 - categorical_accuracy: 0.3512 - val_loss: 5.0838 - val_categorical_accuracy: 0.2100\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.3324 - categorical_accuracy: 0.3491 - val_loss: 6.9531 - val_categorical_accuracy: 0.2300\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 51s 2s/step - loss: 1.4490 - categorical_accuracy: 0.3522 - val_loss: 3.9769 - val_categorical_accuracy: 0.1200\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.4264 - categorical_accuracy: 0.2828 - val_loss: 2.2255 - val_categorical_accuracy: 0.2000\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 51s 1s/step - loss: 1.5316 - categorical_accuracy: 0.3072 - val_loss: 3.1465 - val_categorical_accuracy: 0.1400\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 51s 1s/step - loss: 1.3730 - categorical_accuracy: 0.3196 - val_loss: 3.3615 - val_categorical_accuracy: 0.2200\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.3933 - categorical_accuracy: 0.3575 - val_loss: 2.4924 - val_categorical_accuracy: 0.1700\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.4120 - categorical_accuracy: 0.3346 - val_loss: 2.5616 - val_categorical_accuracy: 0.2600\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.3997 - categorical_accuracy: 0.3363 - val_loss: 1.6136 - val_categorical_accuracy: 0.2800\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.5161 - categorical_accuracy: 0.3429 - val_loss: 1.6373 - val_categorical_accuracy: 0.2000\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.5021 - categorical_accuracy: 0.3019 - val_loss: 1.6455 - val_categorical_accuracy: 0.2500\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.4441 - categorical_accuracy: 0.3080 - val_loss: 1.6781 - val_categorical_accuracy: 0.3700\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.4540 - categorical_accuracy: 0.3309 - val_loss: 2.2142 - val_categorical_accuracy: 0.2600\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.4662 - categorical_accuracy: 0.2679 - val_loss: 1.8489 - val_categorical_accuracy: 0.3600\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.4935 - categorical_accuracy: 0.3293 - val_loss: 1.9494 - val_categorical_accuracy: 0.2500\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.4688 - categorical_accuracy: 0.2834 - val_loss: 1.8807 - val_categorical_accuracy: 0.3600\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 1.6516 - categorical_accuracy: 0.3434 - val_loss: 1.8502 - val_categorical_accuracy: 0.4300\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 51s 1s/step - loss: 1.4648 - categorical_accuracy: 0.3588 - val_loss: 1.8939 - val_categorical_accuracy: 0.4000\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.5456 - categorical_accuracy: 0.2981 - val_loss: 1.6194 - val_categorical_accuracy: 0.4000\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 51s 2s/step - loss: 1.3748 - categorical_accuracy: 0.3657 - val_loss: 1.7560 - val_categorical_accuracy: 0.5300\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.3932 - categorical_accuracy: 0.3409 - val_loss: 1.9991 - val_categorical_accuracy: 0.4600\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.4337 - categorical_accuracy: 0.3433 - val_loss: 1.8488 - val_categorical_accuracy: 0.4800\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.4163 - categorical_accuracy: 0.3805 - val_loss: 1.6676 - val_categorical_accuracy: 0.4500\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.3987 - categorical_accuracy: 0.3367 - val_loss: 1.8806 - val_categorical_accuracy: 0.3600\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 50s 1s/step - loss: 1.3923 - categorical_accuracy: 0.3606 - val_loss: 4.4938 - val_categorical_accuracy: 0.3700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4c782b1be0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for both training and validation data is very low.\n",
    "\n",
    "Next, let's change x,y and z and reduce the number of epochs to 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 :  Experiment Number 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 21 # number of frames\n",
    "y = 84 # image width\n",
    "z = 84 # image height\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx =[0,1,2,3,4,6,8,10,12,14,15,16,18,20,22,24,25,26,27,28,29] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches =len(t)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=np.array(Image.fromarray((image * 255).astype(np.uint8)).resize((84, 84)).convert('RGB'))                    \n",
    "                    batch_data[folder,idx,:,:,0] =  (image[:,:,0])/255#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] =  (image[:,:,1])/255#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] =  (image[:,:,2])/255#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if(len(t)%batch_size)!=0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(len(t)%batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=np.array(Image.fromarray((image * 255).astype(np.uint8)).resize((84, 84)).convert('RGB'))\n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255#normalise and feed in the image # divide by 255.0\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1 # OHE\n",
    "            yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (21,84,84,3)\n",
    "\n",
    "#write your model here\n",
    "model_2 = Sequential()\n",
    "model_2.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "model_2.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model_2.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "model_2.add(Conv3D(512, (3,3,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "#Flatten Layers\n",
    "model_2.add(Flatten())\n",
    "\n",
    "model_2.add(Dense(1000, activation='relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "\n",
    "model_2.add(Dense(500, activation='relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model_2.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_3 (Conv3D)            (None, 21, 84, 84, 64)    5248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 21, 84, 84, 64)    256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 21, 84, 84, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 10, 42, 84, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 10, 42, 84, 128)   221312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 10, 42, 84, 128)   512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10, 42, 84, 128)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 5, 21, 42, 128)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 5, 21, 42, 256)    884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 5, 21, 42, 256)    1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5, 21, 42, 256)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 2, 10, 42, 256)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 2, 10, 42, 512)    3539456   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2, 10, 42, 512)    2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2, 10, 42, 512)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 1, 5, 42, 512)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 107520)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              107521000 \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 112,678,853\n",
      "Trainable params: 112,676,933\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser =  optimizers.Adam(lr=0.001) # write your optimizer\n",
    "model_2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 102.1017 - categorical_accuracy: 0.2433Source path =  Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 145s 4s/step - loss: 101.2555 - categorical_accuracy: 0.2433 - val_loss: 16.8821 - val_categorical_accuracy: 0.2400\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 49s 1s/step - loss: 22.2616 - categorical_accuracy: 0.3000 - val_loss: 2.4095 - val_categorical_accuracy: 0.1700\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 48s 1s/step - loss: 9.6193 - categorical_accuracy: 0.3004 - val_loss: 1.9910 - val_categorical_accuracy: 0.1800\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 48s 1s/step - loss: 7.7487 - categorical_accuracy: 0.2877 - val_loss: 2.0298 - val_categorical_accuracy: 0.1200\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 49s 1s/step - loss: 5.1722 - categorical_accuracy: 0.3552 - val_loss: 1.7248 - val_categorical_accuracy: 0.2200\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 48s 1s/step - loss: 3.1766 - categorical_accuracy: 0.3088 - val_loss: 1.6530 - val_categorical_accuracy: 0.2200\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 48s 1s/step - loss: 4.0812 - categorical_accuracy: 0.3462 - val_loss: 1.7933 - val_categorical_accuracy: 0.1600\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 48s 1s/step - loss: 1.9245 - categorical_accuracy: 0.2902 - val_loss: 1.6145 - val_categorical_accuracy: 0.1200\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 47s 1s/step - loss: 1.6171 - categorical_accuracy: 0.2032 - val_loss: 1.6038 - val_categorical_accuracy: 0.2800\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 48s 1s/step - loss: 1.5978 - categorical_accuracy: 0.2242 - val_loss: 1.6080 - val_categorical_accuracy: 0.1900\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 48s 1s/step - loss: 1.5991 - categorical_accuracy: 0.2117 - val_loss: 1.6088 - val_categorical_accuracy: 0.2100\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 48s 1s/step - loss: 1.6050 - categorical_accuracy: 0.2678 - val_loss: 1.6100 - val_categorical_accuracy: 0.1900\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 47s 1s/step - loss: 1.6061 - categorical_accuracy: 0.2267 - val_loss: 1.6078 - val_categorical_accuracy: 0.2200\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 47s 1s/step - loss: 1.6100 - categorical_accuracy: 0.2158 - val_loss: 1.6078 - val_categorical_accuracy: 0.2200\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 47s 1s/step - loss: 1.6086 - categorical_accuracy: 0.2001 - val_loss: 1.6088 - val_categorical_accuracy: 0.1900\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 47s 1s/step - loss: 1.6081 - categorical_accuracy: 0.2171 - val_loss: 1.6079 - val_categorical_accuracy: 0.2100\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 47s 1s/step - loss: 1.6102 - categorical_accuracy: 0.1879 - val_loss: 1.6072 - val_categorical_accuracy: 0.2000\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 47s 1s/step - loss: 1.6081 - categorical_accuracy: 0.2185 - val_loss: 1.6090 - val_categorical_accuracy: 0.2000\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 47s 1s/step - loss: 1.6253 - categorical_accuracy: 0.2078 - val_loss: 1.6099 - val_categorical_accuracy: 0.1600\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 48s 1s/step - loss: 1.6086 - categorical_accuracy: 0.1856 - val_loss: 1.6058 - val_categorical_accuracy: 0.2700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3c1c418e50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment:\n",
    "\n",
    "Performance has dropped from previous model. We will retain the image size as 120x120 and reduce the filter size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 :  Experiment Number 2\n",
    "- Keep image size as 120x120 and reduce the filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 21 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx =[0,1,2,3,4,6,8,10,12,14,15,16,18,20,22,24,25,26,27,28,29] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches =len(t)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=np.array(Image.fromarray((image * 255).astype(np.uint8)).resize((120, 120)).convert('RGB'))                    \n",
    "                    batch_data[folder,idx,:,:,0] =  (image[:,:,0])/255#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] =  (image[:,:,1])/255#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] =  (image[:,:,2])/255#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if(len(t)%batch_size)!=0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(len(t)%batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=np.array(Image.fromarray((image * 255).astype(np.uint8)).resize((120, 120)).convert('RGB'))\n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255#normalise and feed in the image # divide by 255.0\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1 # OHE\n",
    "            yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (21,120,120,3)\n",
    "\n",
    "#write your model here\n",
    "model_3 = Sequential()\n",
    "model_3.add(Conv3D(64, (2,2,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,1)))\n",
    "\n",
    "model_3.add(Conv3D(128, (2,2,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model_3.add(Conv3D(256, (2,2,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,1)))\n",
    "\n",
    "model_3.add(Conv3D(512, (2,2,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,1)))\n",
    "\n",
    "#Flatten Layers\n",
    "model_3.add(Flatten())\n",
    "\n",
    "model_3.add(Dense(1000, activation='relu'))\n",
    "model_3.add(Dropout(0.5))\n",
    "\n",
    "model_3.add(Dense(500, activation='relu'))\n",
    "model_3.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model_3.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_7 (Conv3D)            (None, 21, 120, 120, 64)  2368      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 21, 120, 120, 64)  256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 21, 120, 120, 64)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 10, 60, 119, 64)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 10, 60, 119, 128)  98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 10, 60, 119, 128)  512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10, 60, 119, 128)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 5, 30, 59, 128)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 5, 30, 59, 256)    393472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 5, 30, 59, 256)    1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5, 30, 59, 256)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 2, 15, 58, 256)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 2, 15, 58, 512)    1573376   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 2, 15, 58, 512)    2048      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2, 15, 58, 512)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 1, 7, 57, 512)     0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 204288)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              204289000 \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 206,863,493\n",
      "Trainable params: 206,861,573\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser =  optimizers.Adam(lr=0.001) # write your optimizer\n",
    "model_3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 165.9387 - categorical_accuracy: 0.2834Source path =  Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 65s 2s/step - loss: 164.4209 - categorical_accuracy: 0.2833 - val_loss: 5.3459 - val_categorical_accuracy: 0.2900\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 53s 2s/step - loss: 2.2921 - categorical_accuracy: 0.2668 - val_loss: 6.1360 - val_categorical_accuracy: 0.2300\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 53s 2s/step - loss: 1.4222 - categorical_accuracy: 0.3656 - val_loss: 2.1139 - val_categorical_accuracy: 0.2300\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 54s 2s/step - loss: 1.2363 - categorical_accuracy: 0.4492 - val_loss: 2.1521 - val_categorical_accuracy: 0.2300\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 53s 2s/step - loss: 1.1444 - categorical_accuracy: 0.4858 - val_loss: 2.5184 - val_categorical_accuracy: 0.2800\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 54s 2s/step - loss: 1.0101 - categorical_accuracy: 0.5613 - val_loss: 2.2719 - val_categorical_accuracy: 0.1500\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 53s 2s/step - loss: 1.0459 - categorical_accuracy: 0.5485 - val_loss: 2.7716 - val_categorical_accuracy: 0.3000\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 54s 2s/step - loss: 0.9814 - categorical_accuracy: 0.5708 - val_loss: 2.4495 - val_categorical_accuracy: 0.2100\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 54s 2s/step - loss: 0.9327 - categorical_accuracy: 0.5907 - val_loss: 3.0217 - val_categorical_accuracy: 0.1400\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 54s 2s/step - loss: 0.7938 - categorical_accuracy: 0.6875 - val_loss: 2.8177 - val_categorical_accuracy: 0.1700\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 54s 2s/step - loss: 0.7036 - categorical_accuracy: 0.7279 - val_loss: 2.3701 - val_categorical_accuracy: 0.2400\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.7223 - categorical_accuracy: 0.6875 - val_loss: 2.0234 - val_categorical_accuracy: 0.2300\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 53s 2s/step - loss: 0.6452 - categorical_accuracy: 0.7625 - val_loss: 1.7518 - val_categorical_accuracy: 0.3400\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 54s 2s/step - loss: 0.6382 - categorical_accuracy: 0.7955 - val_loss: 1.7380 - val_categorical_accuracy: 0.4500\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 54s 2s/step - loss: 0.5470 - categorical_accuracy: 0.7970 - val_loss: 1.5230 - val_categorical_accuracy: 0.5000\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 53s 2s/step - loss: 0.4752 - categorical_accuracy: 0.8338 - val_loss: 1.0048 - val_categorical_accuracy: 0.6100\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 53s 2s/step - loss: 0.5109 - categorical_accuracy: 0.8062 - val_loss: 0.9244 - val_categorical_accuracy: 0.6800\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 53s 2s/step - loss: 0.4403 - categorical_accuracy: 0.8386 - val_loss: 1.3991 - val_categorical_accuracy: 0.5600\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 54s 2s/step - loss: 0.4233 - categorical_accuracy: 0.8574 - val_loss: 1.0957 - val_categorical_accuracy: 0.6400\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.3794 - categorical_accuracy: 0.8490 - val_loss: 0.6869 - val_categorical_accuracy: 0.7300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3c206cc640>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "The model is still not able to generalise well on the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Experiment Number 3\n",
    "\n",
    "We will increase the batch_size from 20 to 40 and see the model performance\n",
    "\n",
    "- When the batch size was increased from 20 to 40, we observed OOM error.\n",
    "- On further reducing the batch_size, it was observed that 23 is the max batch_size which the GPU machine accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, 23)\n",
    "val_generator = generator(val_path, val_doc, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 23\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 129.6199 - categorical_accuracy: 0.2621Source path =  Project_data/val ; batch size = 23\n",
      "34/34 [==============================] - 164s 4s/step - loss: 128.2598 - categorical_accuracy: 0.2630 - val_loss: 17.2761 - val_categorical_accuracy: 0.2100\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 1.8419 - categorical_accuracy: 0.3027 - val_loss: 1.6877 - val_categorical_accuracy: 0.2400\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 1.2224 - categorical_accuracy: 0.4677 - val_loss: 1.7947 - val_categorical_accuracy: 0.2700\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 1.1859 - categorical_accuracy: 0.4672 - val_loss: 1.7521 - val_categorical_accuracy: 0.1600\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.0079 - categorical_accuracy: 0.5871 - val_loss: 1.7296 - val_categorical_accuracy: 0.2600\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.9345 - categorical_accuracy: 0.6004 - val_loss: 1.7281 - val_categorical_accuracy: 0.1900\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.8390 - categorical_accuracy: 0.6538 - val_loss: 1.7754 - val_categorical_accuracy: 0.2100\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.7411 - categorical_accuracy: 0.7241 - val_loss: 1.8362 - val_categorical_accuracy: 0.2600\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.5865 - categorical_accuracy: 0.7761 - val_loss: 1.9807 - val_categorical_accuracy: 0.2200\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.5706 - categorical_accuracy: 0.7618 - val_loss: 1.8866 - val_categorical_accuracy: 0.2400\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.5302 - categorical_accuracy: 0.7946 - val_loss: 2.0471 - val_categorical_accuracy: 0.2500\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.4172 - categorical_accuracy: 0.8327 - val_loss: 2.1835 - val_categorical_accuracy: 0.4100\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.3930 - categorical_accuracy: 0.8429 - val_loss: 2.4905 - val_categorical_accuracy: 0.3600\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.3295 - categorical_accuracy: 0.8698 - val_loss: 1.6395 - val_categorical_accuracy: 0.4500\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.2603 - categorical_accuracy: 0.8950 - val_loss: 1.3327 - val_categorical_accuracy: 0.5800\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.2805 - categorical_accuracy: 0.8854 - val_loss: 1.2867 - val_categorical_accuracy: 0.6500\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.1890 - categorical_accuracy: 0.9188 - val_loss: 1.1799 - val_categorical_accuracy: 0.7000\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2935 - categorical_accuracy: 0.9054 - val_loss: 1.2374 - val_categorical_accuracy: 0.6500\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2729 - categorical_accuracy: 0.9155 - val_loss: 1.1914 - val_categorical_accuracy: 0.6200\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1720 - categorical_accuracy: 0.9372 - val_loss: 1.5410 - val_categorical_accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f666802f610>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment:\n",
    "Although, Accuracy on the train set has increased, the model is not performing well on validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Experiment Number 4\n",
    "\n",
    "In this experiment, we will use SGD optimiser and observe the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_7 (Conv3D)            (None, 21, 120, 120, 64)  2368      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 21, 120, 120, 64)  256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 21, 120, 120, 64)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 10, 60, 119, 64)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 10, 60, 119, 128)  98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 10, 60, 119, 128)  512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10, 60, 119, 128)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 5, 30, 59, 128)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 5, 30, 59, 256)    393472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 5, 30, 59, 256)    1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5, 30, 59, 256)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 2, 15, 58, 256)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 2, 15, 58, 512)    1573376   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 2, 15, 58, 512)    2048      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2, 15, 58, 512)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 1, 7, 57, 512)     0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 204288)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              204289000 \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 206,863,493\n",
      "Trainable params: 206,861,573\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7)\n",
    "model_3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, 23)\n",
    "val_generator = generator(val_path, val_doc, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 23\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1607 - categorical_accuracy: 0.9498Source path =  Project_data/val ; batch size = 23\n",
      "34/34 [==============================] - 64s 2s/step - loss: 0.1609 - categorical_accuracy: 0.9496 - val_loss: 7.7006 - val_categorical_accuracy: 0.2800\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1752 - categorical_accuracy: 0.9245 - val_loss: 1.7093 - val_categorical_accuracy: 0.6800\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.1167 - categorical_accuracy: 0.9571 - val_loss: 1.2959 - val_categorical_accuracy: 0.7300\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.1015 - categorical_accuracy: 0.9596 - val_loss: 1.5252 - val_categorical_accuracy: 0.6500\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1671 - categorical_accuracy: 0.9397 - val_loss: 4.6884 - val_categorical_accuracy: 0.3100\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1575 - categorical_accuracy: 0.9488 - val_loss: 1.4310 - val_categorical_accuracy: 0.6300\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.1094 - categorical_accuracy: 0.9682 - val_loss: 1.3474 - val_categorical_accuracy: 0.6500\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.1011 - categorical_accuracy: 0.9751 - val_loss: 1.5801 - val_categorical_accuracy: 0.6200\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.1343 - categorical_accuracy: 0.9502 - val_loss: 1.1450 - val_categorical_accuracy: 0.6600\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.0726 - categorical_accuracy: 0.9774 - val_loss: 2.7911 - val_categorical_accuracy: 0.5700\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.0981 - categorical_accuracy: 0.9614 - val_loss: 1.3367 - val_categorical_accuracy: 0.6600\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.0749 - categorical_accuracy: 0.9690 - val_loss: 1.4450 - val_categorical_accuracy: 0.6200\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.0883 - categorical_accuracy: 0.9682 - val_loss: 0.8523 - val_categorical_accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.0741 - categorical_accuracy: 0.9760 - val_loss: 1.5091 - val_categorical_accuracy: 0.7000\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1003 - categorical_accuracy: 0.9594 - val_loss: 1.0741 - val_categorical_accuracy: 0.7400\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.0864 - categorical_accuracy: 0.9690 - val_loss: 1.4778 - val_categorical_accuracy: 0.6900\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.0730 - categorical_accuracy: 0.9782 - val_loss: 1.5359 - val_categorical_accuracy: 0.7100\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.0896 - categorical_accuracy: 0.9614 - val_loss: 1.3343 - val_categorical_accuracy: 0.6800\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.0501 - categorical_accuracy: 0.9938 - val_loss: 1.6377 - val_categorical_accuracy: 0.6700\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.0810 - categorical_accuracy: 0.9691 - val_loss: 1.1518 - val_categorical_accuracy: 0.6900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f660c7c8880>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Comment:\n",
    "\n",
    "The model is overfitting. Next, we will try to change the number of images, filtersize and number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - Experiment Number 5\n",
    "\n",
    "- Using dropout and number of epochs = 25\n",
    "- Reducing number of images and changing the number of features in Conv3D layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (16,120,120,3)\n",
    "\n",
    "#write your model here\n",
    "model_4= Sequential()\n",
    "model_4.add(Conv3D(16, (2,2,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,1)))\n",
    "\n",
    "model_4.add(Conv3D(32, (2,2,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model_4.add(Conv3D(64, (2,2,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,1)))\n",
    "\n",
    "model_4.add(Conv3D(128, (2,2,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,1)))\n",
    "model_4.add(Dropout(0.25))\n",
    "\n",
    "#Flatten Layers\n",
    "model_4.add(Flatten())\n",
    "\n",
    "model_4.add(Dense(256, activation='relu'))\n",
    "model_4.add(Dropout(0.25))\n",
    "\n",
    "model_4.add(Dense(128, activation='relu'))\n",
    "model_4.add(Dropout(0.25))\n",
    "\n",
    "#softmax layer\n",
    "model_4.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_3 (Conv3D)            (None, 16, 120, 120, 16)  592       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 8, 60, 119, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 8, 60, 119, 32)    6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 60, 119, 32)    128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 60, 119, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 4, 30, 59, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 4, 30, 59, 64)     24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 30, 59, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4, 30, 59, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 2, 15, 58, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 2, 15, 58, 128)    98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2, 15, 58, 128)    512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2, 15, 58, 128)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 1, 7, 57, 128)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 7, 57, 128)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 51072)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               13074688  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 13,239,029\n",
      "Trainable params: 13,238,549\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser =  optimizers.Adam(lr=0.001) # write your optimizer\n",
    "model_4.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, 20)\n",
    "val_generator = generator(val_path, val_doc, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 20\n",
      "Epoch 1/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 18.5482 - categorical_accuracy: 0.2627Source path =  Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 135s 4s/step - loss: 18.3574 - categorical_accuracy: 0.2638 - val_loss: 2.1463 - val_categorical_accuracy: 0.1700\n",
      "Epoch 2/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 2.8783 - categorical_accuracy: 0.3841 - val_loss: 2.3272 - val_categorical_accuracy: 0.1300\n",
      "Epoch 3/25\n",
      "34/34 [==============================] - 43s 1s/step - loss: 1.7994 - categorical_accuracy: 0.3075 - val_loss: 1.9584 - val_categorical_accuracy: 0.1600\n",
      "Epoch 4/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 1.3046 - categorical_accuracy: 0.4288 - val_loss: 2.5706 - val_categorical_accuracy: 0.2100\n",
      "Epoch 5/25\n",
      "34/34 [==============================] - 45s 1s/step - loss: 1.1352 - categorical_accuracy: 0.5124 - val_loss: 2.4604 - val_categorical_accuracy: 0.2000\n",
      "Epoch 6/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 1.2488 - categorical_accuracy: 0.4263 - val_loss: 2.4004 - val_categorical_accuracy: 0.1600\n",
      "Epoch 7/25\n",
      "34/34 [==============================] - 45s 1s/step - loss: 0.9814 - categorical_accuracy: 0.5870 - val_loss: 3.0305 - val_categorical_accuracy: 0.1600\n",
      "Epoch 8/25\n",
      "34/34 [==============================] - 47s 1s/step - loss: 0.8174 - categorical_accuracy: 0.6719 - val_loss: 2.5299 - val_categorical_accuracy: 0.2300\n",
      "Epoch 9/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.9109 - categorical_accuracy: 0.6340 - val_loss: 1.4737 - val_categorical_accuracy: 0.3800\n",
      "Epoch 10/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.7521 - categorical_accuracy: 0.6788 - val_loss: 1.5562 - val_categorical_accuracy: 0.3500\n",
      "Epoch 11/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.7145 - categorical_accuracy: 0.7210 - val_loss: 0.9857 - val_categorical_accuracy: 0.5400\n",
      "Epoch 12/25\n",
      "34/34 [==============================] - 45s 1s/step - loss: 0.7116 - categorical_accuracy: 0.7250 - val_loss: 1.3321 - val_categorical_accuracy: 0.4800\n",
      "Epoch 13/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.6779 - categorical_accuracy: 0.7485 - val_loss: 0.8330 - val_categorical_accuracy: 0.6500\n",
      "Epoch 14/25\n",
      "34/34 [==============================] - 45s 1s/step - loss: 0.6446 - categorical_accuracy: 0.7686 - val_loss: 1.0125 - val_categorical_accuracy: 0.7000\n",
      "Epoch 15/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.6035 - categorical_accuracy: 0.7853 - val_loss: 0.7447 - val_categorical_accuracy: 0.7500\n",
      "Epoch 16/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.4374 - categorical_accuracy: 0.8329 - val_loss: 1.0175 - val_categorical_accuracy: 0.6300\n",
      "Epoch 17/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.3967 - categorical_accuracy: 0.8619 - val_loss: 0.9533 - val_categorical_accuracy: 0.7600\n",
      "Epoch 18/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.4327 - categorical_accuracy: 0.8250 - val_loss: 1.1099 - val_categorical_accuracy: 0.6400\n",
      "Epoch 19/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.3153 - categorical_accuracy: 0.8843 - val_loss: 0.9815 - val_categorical_accuracy: 0.6600\n",
      "Epoch 20/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.3327 - categorical_accuracy: 0.8764 - val_loss: 0.8569 - val_categorical_accuracy: 0.7800\n",
      "Epoch 21/25\n",
      "34/34 [==============================] - 45s 1s/step - loss: 0.2501 - categorical_accuracy: 0.9169 - val_loss: 1.1822 - val_categorical_accuracy: 0.7300\n",
      "Epoch 22/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.4038 - categorical_accuracy: 0.8786 - val_loss: 1.8247 - val_categorical_accuracy: 0.5600\n",
      "Epoch 23/25\n",
      "34/34 [==============================] - 45s 1s/step - loss: 0.2696 - categorical_accuracy: 0.8996 - val_loss: 1.1546 - val_categorical_accuracy: 0.7000\n",
      "Epoch 24/25\n",
      "34/34 [==============================] - 45s 1s/step - loss: 0.2566 - categorical_accuracy: 0.9109 - val_loss: 1.1302 - val_categorical_accuracy: 0.7300\n",
      "Epoch 25/25\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.2294 - categorical_accuracy: 0.9159 - val_loss: 1.4732 - val_categorical_accuracy: 0.7600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7480043820>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs= 25, verbose=1, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment:\n",
    "\n",
    "After few experiments, we observe that the model is still not able to generalize well on the validation set. Let's next use **Transfer Learning+RNN**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using transfer learning+RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5 - VGG16 + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 21 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    # It is not possible to work with all the 30 images, as it will take too long processing time.\n",
    "    # So lets choose randomly 21 images, as this is more computationally expensive\n",
    "    img_idx = [0,1,2,3,4,6,10,12,13,14,16,17,18,20,21,22,24,25,27,28,29] #create a list of image numbers you want to use for a particular video(incase if u want to try with lesser images)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,21,120,120,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=np.array(Image.fromarray((image * 255).astype(np.uint8)).resize((120, 120)).convert('RGB'))\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1 # OHE\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if(len(t)%batch_size)!=0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,21,120,120,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(len(t)%batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=np.array(Image.fromarray((image * 255).astype(np.uint8)).resize((120, 120)).convert('RGB'))\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1 # OHE\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout,GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(include_top=False, weights = 'imagenet', input_shape = (120,120,3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "features = Dense(64, activation='relu')(x)\n",
    "conv_model = Model(inputs= base_model.input, outputs=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_model = Sequential()\n",
    "cnn_rnn_model.add(TimeDistributed(conv_model, input_shape=(21,120,120,3)))\n",
    "cnn_rnn_model.add(TimeDistributed(BatchNormalization()))\n",
    "cnn_rnn_model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "cnn_rnn_model.add(GRU(256, activation = 'relu', return_sequences=True))\n",
    "cnn_rnn_model.add(GRU(64))\n",
    "cnn_rnn_model.add(Dropout(0.5))\n",
    "cnn_rnn_model.add(Dense(8, activation='relu'))\n",
    "cnn_rnn_model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_3 (TimeDist (None, 21, 64)            15009664  \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 21, 256)           247296    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64)                61824     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 45        \n",
      "=================================================================\n",
      "Total params: 15,319,605\n",
      "Trainable params: 604,789\n",
      "Non-trainable params: 14,714,816\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser =  optimizers.Adam(lr=0.001) # write your optimizer\n",
    "cnn_rnn_model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (cnn_rnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if #images=55, batch_size=5, then we have 11 batches and 11 epochs\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 20\n",
      "Epoch 1/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5763 - categorical_accuracy: 0.2687Source path =  Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 160s 4s/step - loss: 1.5746 - categorical_accuracy: 0.2701 - val_loss: 1.5693 - val_categorical_accuracy: 0.2600\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 50s 2s/step - loss: 1.0972 - categorical_accuracy: 0.5352 - val_loss: 1.2816 - val_categorical_accuracy: 0.5100\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.8782 - categorical_accuracy: 0.6429 - val_loss: 0.9376 - val_categorical_accuracy: 0.6200\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.5337 - categorical_accuracy: 0.8185 - val_loss: 1.5166 - val_categorical_accuracy: 0.4700\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.4047 - categorical_accuracy: 0.8778 - val_loss: 1.2352 - val_categorical_accuracy: 0.5900\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.3226 - categorical_accuracy: 0.9135 - val_loss: 1.0495 - val_categorical_accuracy: 0.6600\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.2297 - categorical_accuracy: 0.9232 - val_loss: 0.9977 - val_categorical_accuracy: 0.7200\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.1737 - categorical_accuracy: 0.9577 - val_loss: 1.1300 - val_categorical_accuracy: 0.6900\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 48s 1s/step - loss: 0.1719 - categorical_accuracy: 0.9534 - val_loss: 1.2624 - val_categorical_accuracy: 0.7100\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.1712 - categorical_accuracy: 0.9540 - val_loss: 1.5742 - val_categorical_accuracy: 0.6400\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.1533 - categorical_accuracy: 0.9457 - val_loss: 1.1702 - val_categorical_accuracy: 0.7200\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.1392 - categorical_accuracy: 0.9615 - val_loss: 1.3829 - val_categorical_accuracy: 0.6400\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.1294 - categorical_accuracy: 0.9653 - val_loss: 1.3190 - val_categorical_accuracy: 0.6400\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.0854 - categorical_accuracy: 0.9805 - val_loss: 1.2850 - val_categorical_accuracy: 0.6900\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 48s 1s/step - loss: 0.0449 - categorical_accuracy: 0.9946 - val_loss: 1.3874 - val_categorical_accuracy: 0.6700\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.1509 - categorical_accuracy: 0.9625 - val_loss: 1.6529 - val_categorical_accuracy: 0.6400\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.0836 - categorical_accuracy: 0.9812 - val_loss: 1.2621 - val_categorical_accuracy: 0.7200\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 48s 1s/step - loss: 0.0761 - categorical_accuracy: 0.9857 - val_loss: 1.3807 - val_categorical_accuracy: 0.7300\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.0954 - categorical_accuracy: 0.9789 - val_loss: 0.8572 - val_categorical_accuracy: 0.8100\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.0600 - categorical_accuracy: 0.9842 - val_loss: 1.1484 - val_categorical_accuracy: 0.7400\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.1334 - categorical_accuracy: 0.9672 - val_loss: 1.5313 - val_categorical_accuracy: 0.6500\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 48s 1s/step - loss: 0.0962 - categorical_accuracy: 0.9740 - val_loss: 1.4633 - val_categorical_accuracy: 0.7400\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.1345 - categorical_accuracy: 0.9682 - val_loss: 1.1805 - val_categorical_accuracy: 0.7400\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.0699 - categorical_accuracy: 0.9849 - val_loss: 0.8856 - val_categorical_accuracy: 0.7800\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.0303 - categorical_accuracy: 0.9948 - val_loss: 1.1452 - val_categorical_accuracy: 0.7400\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.0494 - categorical_accuracy: 0.9915 - val_loss: 1.1946 - val_categorical_accuracy: 0.7400\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.0680 - categorical_accuracy: 0.9756 - val_loss: 1.2738 - val_categorical_accuracy: 0.7200\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.1686 - categorical_accuracy: 0.9544 - val_loss: 1.8205 - val_categorical_accuracy: 0.6500\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.0811 - categorical_accuracy: 0.9763 - val_loss: 1.6488 - val_categorical_accuracy: 0.7200\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 49s 1s/step - loss: 0.0559 - categorical_accuracy: 0.9865 - val_loss: 1.8742 - val_categorical_accuracy: 0.6900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff134110790>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_rnn_model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment:\n",
    "This model is overfitting too. We will next try to use MobileNet which reduces the number of parameters significantly and thus results in lightweight network. MobileNet can train our classifiers faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model -  Model 6 :  MobileNet + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 21 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    # It is not possible to work with all the 30 images, as it will take too long processing time.\n",
    "    # So lets choose randomly 21 images, as this is more computationally expensive\n",
    "    img_idx = [0,1,2,3,4,6,10,12,13,14,16,17,18,20,21,22,24,25,27,28,29] #create a list of image numbers you want to use for a particular video(incase if u want to try with lesser images)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,21,120,120,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=np.array(Image.fromarray((image * 255).astype(np.uint8)).resize((120, 120)).convert('RGB'))\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1 # OHE\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if(len(t)%batch_size)!=0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,21,120,120,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(len(t)%batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image=np.array(Image.fromarray((image * 255).astype(np.uint8)).resize((120, 120)).convert('RGB'))\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1 # OHE\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 20 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import mobilenet\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout,GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "mob_model = Sequential()\n",
    "mob_model.add(TimeDistributed(mobilenet_transfer,input_shape=(21,120,120,3)))\n",
    "\n",
    "mob_model.add(TimeDistributed(BatchNormalization()))\n",
    "mob_model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "mob_model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "mob_model.add(GRU(64))\n",
    "mob_model.add(Dropout(0.3))\n",
    "\n",
    "mob_model.add(Dense(32,activation='relu'))\n",
    "mob_model.add(Dropout(0.3))\n",
    "\n",
    "mob_model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 21, 3, 3, 1024)    3228864   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 21, 3, 3, 1024)    4096      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 21, 1, 1, 1024)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 64)                209280    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 3,444,485\n",
      "Trainable params: 3,420,549\n",
      "Non-trainable params: 23,936\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser =  optimizers.Adam() # write your optimizer\n",
    "mob_model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (mob_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6743 - categorical_accuracy: 0.3115Source path =  Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 195s 5s/step - loss: 1.6695 - categorical_accuracy: 0.3135 - val_loss: 1.3592 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.35924, saving model to model_init_2021-10-2113_35_59.183434/model-00001-1.50738-0.38009-1.35924-0.47000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 53s 2s/step - loss: 1.0552 - categorical_accuracy: 0.5871 - val_loss: 1.0167 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.35924 to 1.01671, saving model to model_init_2021-10-2113_35_59.183434/model-00002-0.97368-0.61538-1.01671-0.56000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.5491 - categorical_accuracy: 0.8027 - val_loss: 0.4173 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01671 to 0.41732, saving model to model_init_2021-10-2113_35_59.183434/model-00003-0.53838-0.81146-0.41732-0.86000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.3866 - categorical_accuracy: 0.8856 - val_loss: 0.4645 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.41732\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.2268 - categorical_accuracy: 0.9403 - val_loss: 0.8224 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.41732\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.2170 - categorical_accuracy: 0.9412 - val_loss: 0.4484 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.41732\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.1730 - categorical_accuracy: 0.9587 - val_loss: 0.5037 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.41732\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.1202 - categorical_accuracy: 0.9638 - val_loss: 0.2918 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.41732 to 0.29178, saving model to model_init_2021-10-2113_35_59.183434/model-00008-0.11938-0.96531-0.29178-0.90000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 51s 2s/step - loss: 0.1199 - categorical_accuracy: 0.9696 - val_loss: 0.2769 - val_categorical_accuracy: 0.8900\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.29178 to 0.27690, saving model to model_init_2021-10-2113_35_59.183434/model-00009-0.11931-0.97134-0.27690-0.89000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.1020 - categorical_accuracy: 0.9720 - val_loss: 0.4030 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.27690\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.1246 - categorical_accuracy: 0.9773 - val_loss: 0.3052 - val_categorical_accuracy: 0.8900\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.27690\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.0881 - categorical_accuracy: 0.9758 - val_loss: 0.3178 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.27690\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.0938 - categorical_accuracy: 0.9780 - val_loss: 0.2999 - val_categorical_accuracy: 0.9200\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.27690\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 53s 2s/step - loss: 0.0567 - categorical_accuracy: 0.9952 - val_loss: 0.2158 - val_categorical_accuracy: 0.9400\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.27690 to 0.21575, saving model to model_init_2021-10-2113_35_59.183434/model-00014-0.06151-0.99246-0.21575-0.94000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.0547 - categorical_accuracy: 0.9985 - val_loss: 0.3343 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.21575\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.0493 - categorical_accuracy: 0.9873 - val_loss: 0.2798 - val_categorical_accuracy: 0.9200\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.21575\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 51s 2s/step - loss: 0.0466 - categorical_accuracy: 0.9991 - val_loss: 0.2688 - val_categorical_accuracy: 0.9100\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.21575\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 51s 2s/step - loss: 0.0459 - categorical_accuracy: 0.9946 - val_loss: 0.2389 - val_categorical_accuracy: 0.9500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.21575\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.0489 - categorical_accuracy: 0.9954 - val_loss: 0.2714 - val_categorical_accuracy: 0.8900\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.21575\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 52s 2s/step - loss: 0.0442 - categorical_accuracy: 0.9962 - val_loss: 0.2857 - val_categorical_accuracy: 0.9200\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.21575\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb2700d0730>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mob_model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs= num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list,validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment:\n",
    "This model is performing well on both training and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "In the final model, we have saved only best epochs by setting save_best_only = True due to space issues/constraints on nimblebox.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**: \n",
    "- Train =  99% \n",
    "- Validation = 94%\n",
    "\n",
    "**loss**:\n",
    "- Train_loss = 5%\n",
    "- Val_loss = 21%\n",
    "\n",
    "**Number of parameters** in the final model has reduced considerably and it is as follows:\n",
    "\n",
    "- Total params: 3,444,485\n",
    "- Trainable params: 3,420,549\n",
    "- Non-trainable params: 23,936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first CNN3D model which we have created performed poorly on both train and val data. Thereafter, following models performance increased by experimenting with few parameters. In this assignment, we learnt to play around with CNN models by changing number of frames, epochs, learning rate(optimiser's), dropouts(increasing and decreasing) and use Transfer Learning.\n",
    "\n",
    "One issue which persisted through various experiments that model was not able to generalise well on the validation set. In order to overcome overfitting, we have included batch normalisation, used dropouts and tweaked learning rate by using ReduceLROnPlateau."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
